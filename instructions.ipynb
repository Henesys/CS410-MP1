{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the MeTA python bindings\n",
    "import metapy\n",
    "#If you'd like, you can tell MeTA to log to stderr so you can get progress output when running long-running function calls.\n",
    "metapy.log_to_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tok.set_content(doc.content()) # this could be any string\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>', '<s>', 'I', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "doc.content(\"I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\")\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'said', 'that', 'I', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', 'I', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'that', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '19.95', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '30', 'before']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.LengthFilter(tok, min=2, max=30)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lemur-stopwords.txt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt\"\n",
    "\n",
    "filename = wget.download(url)\n",
    "\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"can't\", 'believe', 'costs', '19.95', 'find', '30']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.ListFilter(tok, \"lemur-stopwords.txt\", metapy.analyzers.ListFilter.Type.Reject)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"can't\", 'believ', 'cost', '19.95', 'find', '30']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.Porter2Filter(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'said', 'that', 'i', \"can't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', 'i', 'could', 'only', 'find', 'it', 'for', 'more', 'than', '$', '30', 'before', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.ICUTokenizer(suppress_tags=True)\n",
    "tok = metapy.analyzers.LowercaseFilter(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I said that I can't believe that it only costs $19.95! I could only find it for more than $30 before.\n",
      "{'30': 1, '.': 1, 'more': 1, 'it': 2, 'said': 1, 'only': 2, \"can't\": 1, 'believe': 1, '19.95': 1, 'before': 1, 'could': 1, 'find': 1, 'that': 2, 'than': 1, '!': 1, 'for': 1, 'i': 3, '$': 2, 'costs': 1}\n"
     ]
    }
   ],
   "source": [
    "ana = metapy.analyzers.NGramWordAnalyzer(1, tok)\n",
    "print(doc.content())\n",
    "unigrams = ana.analyze(doc)\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('it', 'only'): 1, ('19.95', '!'): 1, ('only', 'costs'): 1, ('it', 'for'): 1, ('than', '$'): 1, ('$', '19.95'): 1, ('believe', 'that'): 1, ('$', '30'): 1, ('!', 'i'): 1, ('30', 'before'): 1, ('more', 'than'): 1, ('i', \"can't\"): 1, ('find', 'it'): 1, ('for', 'more'): 1, ('before', '.'): 1, (\"can't\", 'believe'): 1, ('that', 'i'): 1, ('i', 'said'): 1, ('that', 'it'): 1, ('only', 'find'): 1, ('could', 'only'): 1, ('said', 'that'): 1, ('costs', '$'): 1, ('i', 'could'): 1}\n"
     ]
    }
   ],
   "source": [
    "ana = metapy.analyzers.NGramWordAnalyzer(2, tok)\n",
    "bigrams = ana.analyze(doc)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(' ', 'o', 'n', 'l'): 2, ('s', 't', 's', ' '): 1, ('t', 'h', 'a', 't'): 2, ('t', ' ', 'I', ' '): 1, ('i', 'n', 'd', ' '): 1, ('h', 'a', 'n', ' '): 1, ('m', 'o', 'r', 'e'): 1, ('c', 'a', 'n', \"'\"): 1, ('$', '3', '0', ' '): 1, ('n', 'd', ' ', 'i'): 1, ('t', ' ', 'b', 'e'): 1, ('I', ' ', 's', 'a'): 1, ('a', 'i', 'd', ' '): 1, ('l', 'y', ' ', 'c'): 1, ('r', 'e', ' ', 't'): 1, ('l', 'y', ' ', 'f'): 1, ('e', 'l', 'i', 'e'): 1, ('o', 'r', ' ', 'm'): 1, ('i', 'd', ' ', 't'): 1, ('$', '1', '9', '.'): 1, ('t', ' ', 'f', 'o'): 1, ('l', 'i', 'e', 'v'): 1, ('d', ' ', 't', 'h'): 1, ('.', '9', '5', '!'): 1, ('t', ' ', 'i', 't'): 1, (\"'\", 't', ' ', 'b'): 1, (' ', 'b', 'e', 'f'): 1, ('9', '.', '9', '5'): 1, ('9', '5', '!', ' '): 1, ('d', ' ', 'o', 'n'): 1, ('3', '0', ' ', 'b'): 1, ('b', 'e', 'f', 'o'): 1, ('f', 'o', 'r', 'e'): 1, ('r', ' ', 'm', 'o'): 1, ('i', 't', ' ', 'o'): 1, (' ', 'c', 'o', 'u'): 1, ('n', ' ', '$', '3'): 1, ('t', ' ', 'o', 'n'): 1, (' ', 'I', ' ', 'c'): 2, ('b', 'e', 'l', 'i'): 1, (' ', '$', '1', '9'): 1, ('f', 'i', 'n', 'd'): 1, ('c', 'o', 'u', 'l'): 1, (' ', 'f', 'o', 'r'): 1, ('u', 'l', 'd', ' '): 1, ('o', 'r', 'e', '.'): 1, ('n', 'l', 'y', ' '): 2, (' ', 'f', 'i', 'n'): 1, (' ', 'b', 'e', 'l'): 1, (' ', 'c', 'a', 'n'): 1, (' ', '$', '3', '0'): 1, ('v', 'e', ' ', 't'): 1, ('o', 'n', 'l', 'y'): 2, ('e', 'f', 'o', 'r'): 1, ('e', 'v', 'e', ' '): 1, ('I', ' ', 'c', 'o'): 1, ('o', 'r', 'e', ' '): 1, ('i', 'e', 'v', 'e'): 1, ('y', ' ', 'f', 'i'): 1, ('n', \"'\", 't', ' '): 1, (' ', 'm', 'o', 'r'): 1, ('a', 't', ' ', 'I'): 1, ('a', 'n', \"'\", 't'): 1, ('5', '!', ' ', 'I'): 1, ('o', 'u', 'l', 'd'): 1, ('!', ' ', 'I', ' '): 1, ('a', 'n', ' ', '$'): 1, ('t', 's', ' ', '$'): 1, ('i', 't', ' ', 'f'): 1, ('e', ' ', 't', 'h'): 2, ('s', 'a', 'i', 'd'): 1, ('I', ' ', 'c', 'a'): 1, ('c', 'o', 's', 't'): 1, ('o', 's', 't', 's'): 1, ('a', 't', ' ', 'i'): 1, ('h', 'a', 't', ' '): 2, ('s', ' ', '$', '1'): 1, ('1', '9', '.', '9'): 1, ('0', ' ', 'b', 'e'): 1, ('t', 'h', 'a', 'n'): 1, ('f', 'o', 'r', ' '): 1, (' ', 't', 'h', 'a'): 3, (' ', 's', 'a', 'i'): 1, (' ', 'c', 'o', 's'): 1, ('d', ' ', 'i', 't'): 1, ('l', 'd', ' ', 'o'): 1, ('y', ' ', 'c', 'o'): 1, (' ', 'i', 't', ' '): 2}\n"
     ]
    }
   ],
   "source": [
    "tok = metapy.analyzers.CharacterTokenizer()\n",
    "ana = metapy.analyzers.NGramWordAnalyzer(4, tok)\n",
    "fourchar_ngrams = ana.analyze(doc)\n",
    "print(fourchar_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = metapy.sequence.Sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(The, ???), (dog, ???), (ran, ???), (across, ???), (the, ???), (park, ???), (., ???)\n"
     ]
    }
   ],
   "source": [
    "for word in [\"The\", \"dog\", \"ran\", \"across\", \"the\", \"park\", \".\"]:\n",
    "    seq.add_symbol(word)\n",
    "\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wget @ POS Tagging Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget -nc https://github.com/meta-toolkit/meta/releases/download/v3.0.1/greedy-perceptron-tagger.tar.gz\n",
    "# tar xvf greedy-perceptron-tagger.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = metapy.analyzers.ListFilter(tok, \"lemur-stopwords.txt\", metapy.analyzers.ListFilter.Type.Reject)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'I', 'said', 'that', 'I', 'ca', \"n't\", 'believe', 'that', 'it', 'only', 'costs', '$', '19.95', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "tok = metapy.analyzers.ICUTokenizer() # keep sentence boundaries!\n",
    "tok = metapy.analyzers.PennTreebankNormalizer(tok)\n",
    "tok.set_content(doc.content())\n",
    "tokens = [token for token in tok]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-442d35014dca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mextract_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tagger' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_sequences(tok):\n",
    "    sequences = []\n",
    "    for token in tok:\n",
    "        if token == '<s>':\n",
    "            sequences.append(metapy.sequence.Sequence())\n",
    "        elif token != '</s>':\n",
    "            sequences[-1].add_symbol(token)\n",
    "    return sequences\n",
    "\n",
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "tok.set_content(doc.content())\n",
    "for seq in extract_sequences(tok):\n",
    "    tagger.tag(seq)\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "missing feature id mapping: crf/feature.mapping.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-ac704b0f744b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mana\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyzers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'config.toml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I said that I can't believe that it only costs $19.95!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mana\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: missing feature id mapping: crf/feature.mapping.gz"
     ]
    }
   ],
   "source": [
    "ana = metapy.analyzers.load('config.toml')\n",
    "doc = metapy.index.Document()\n",
    "doc.content(\"I said that I can't believe that it only costs $19.95!\")\n",
    "print(ana.analyze(doc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs410",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
